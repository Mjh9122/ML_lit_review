{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Learning representations by back-propagating errors (1986)\n",
    "## Authors: David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams\n",
    "### Notes: Michael Holtz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "They describe a novel learning approach to the weights of neural networks (referred to as neurone-like units). The procedure iteratively adjusts the weights of the connections in the network to minimize a measure between the output vector and a goal vector. They claim that these weight adjustments allow internal units (hidden layers) to represent important features of the \"task domain\". Furthermore \"regularities in the task are captured by the interactions of these units\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "\n",
    "The goal is a general learning algorithm for units that are not directly connected to either the input or the output. Perceptrons had a similar structure, however the weights of internal connections was entirely written by hand, so there was no learning on internal layers. This algorithm boils down to determining when internal neurons should activate in order to map input vectors to their correct output vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network\n",
    "They then describe the simplest form of their procedure, which occurs on a network following these rules:  \n",
    "1) The network begins with an input layer\n",
    "2) The network end with an output layer\n",
    "3) The network contains 0 or more intermediate layers between the input layer and the output layer\n",
    "4) Connections between neurons must only be formed between neurons in different layers\n",
    "5) The input of each neuron is determined by $x_j = \\sum_i y_y w_{ji} + b$ where $x_i$ is the output of a neuron before $x$ with a connection to $x$, $w_ji$ is a learnable weight, and $b$ is a learnable bias.  \n",
    "6) The output of each neuron is determined by $y_j = \\frac{1}{1 + e^{-x_j}}$\n",
    "\n",
    "They then state that the exact functions in 5 and 6 can be edited, so long as the pair have a bounded derivative. They also state that a linear combination of inputs followed by a non-linear output greatly simplifies the learning procedure. A statement which holds true to today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The error metric\n",
    "\n",
    "They define the goal of their algorithm is to minimize the following error function over a finite set of input-output cases. $$ E = \\frac{1}{2} \\sum_c \\sum_y (y_{j, c} - d_{j, c})^2$$  \n",
    "Were E is the total error, c is an index over the training set, y is the predicted output, and d is the desired output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm  \n",
    "\n",
    "The goal is to minimize E via gradient descent. To do so, they compute the partial derivative of E w.r.t. each of the learnable parameters of the network, with is equivalent to the sum of the partial derivatives for each of the input-output cases. Each derivative is computed in two steps. The forward pass is determined via the input-output rules specified above. The backward pass propagates derivatives from the output layer back to the input layer. \n",
    "\n",
    "#### The backward pass\n",
    "We will compute the partial derivative for a single input-output pair. \n",
    "1) Compute the partial derivative w.r.t. each of the neurons of the output layer \n",
    "   $$\\frac{\\partial E}{\\partial y_j} \\frac{1}{2} (y_j - d_j)^2 = (y_j - d_j)$$\n",
    "2) Apply the chain rule to compute the derivative w.r.t $x_j$, the input to $y_j$  \n",
    "    $$\\frac{\\partial E}{\\partial x_j} = \\frac{\\partial E}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial x_j} $$  \n",
    "    We can obtain $\\frac{\\partial y_j}{\\partial x_j}$ by taking the partial derivative of the output equation w.r.t. $x_j$\n",
    "    $$\\frac{\\partial y_j}{\\partial x_j} = y_j(1 - y_j)$$  \n",
    "3) This gives us the error with respect to the total input of the neuron, but the input is itself a linear function, so we can once again use the chain rule to get the derivative w.r.t. $w_{ji}$, which we can then adjust. \n",
    "    $$\\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial x_j} \\cdot \\frac{\\partial x_j}{\\partial w_{ji}} $$ \n",
    "    $$\\frac{\\partial x_j}{\\partial w_{ji}} = y_i$$  \n",
    "    Where $y_i$ is the output of a neuron a previous layer\n",
    "4) To get the partial w.r.t. the next layer, you use the following $$ \\frac{\\partial E}{\\partial y_i} = \\sum_j \\frac{\\partial E}{\\partial x_j} w_{ji}$$\n",
    "5) Repeating these steps allows you to step back one layer at a time until you reach the first weights in the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights\n",
    "Once the partial derivatives with respect to each weight are generated, you can update that weight $w$ such that $\\Delta w = \\epsilon \\frac{\\partial E}{\\partial w}$. Two strategies are noted in the paper. Updating after each input-output pair, and updating after all input-output pairs. Also noted is a modification where the gradient changes the velocity of the weight rather than the position in weight space. \n",
    "$$  \\Delta w = \\epsilon \\frac{\\partial E}{\\partial w(t)} + \\alpha \\frac{\\partial E}{\\partial w(t -1)}$$ \n",
    "t in this case increments each time all input-output pairs are iterated through, and $\\alpha$ is an exponential decay factor, changing how much earlier gradients affect the current one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    " They conclude with a couple of examples (see code), establishing that random weights should be used to initialize the network and pointing out a weakness in this approach. Gradient descent is prone to being caught in local minima, missing out on performance that would be achieved via the global minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example problem  \n",
    "A vector of length 6 is classified as either being symmetric or asymmetric. The example given is a fully connected, feed forward network with 6 input neurons, 2 hidden neurons in one layer, and one output neuron. Neurons 1 - 6 are connected to neurons 7 and 8, which inturn connect to neuron 9, the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input and output functions \n",
    "def input(y_prev, ws, b):\n",
    "    \"\"\"Input function for a neuron\n",
    "\n",
    "    Args:\n",
    "        y_prev (np.array): array of outputs from previous layer\n",
    "        ws (np.array): array of weights \n",
    "        b (float): bias\n",
    "\n",
    "    Returns:\n",
    "        float: total input\n",
    "    \"\"\"\n",
    "    return y_prev @ ws + b\n",
    "def output(x):\n",
    "    \"\"\"Non-linear output function of a neuron\n",
    "\n",
    "    Args:\n",
    "        x (float): input value\n",
    "\n",
    "    Returns:\n",
    "        float: output value\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.e**-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN: \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize weights and biases based on paper's instructions\n",
    "        \"\"\"\n",
    "        self.weights = np.random.uniform(-.3, .3, 14)\n",
    "        self.biases = np.random.uniform(-.3, .3, 3)\n",
    "        self.last_delta_grads = np.zeros(14), np.zeros(3)\n",
    "\n",
    "    \n",
    "    def update_weights(self, weight_grad, bias_grad, e = .1, a= .9):\n",
    "        \"\"\"Update weights and biases based on batch gradient sums and eq. 9 from the paper\n",
    "\n",
    "        Args:\n",
    "            weight_grad (np.array): gradient batch sum for weights\n",
    "            bias_grad (np.array): gradient batch sum for biases\n",
    "            e (float, optional): gradient multiplier (learning rate). Defaults to .1.\n",
    "            a (float, optional): exponential decay factor. Defaults to .9.\n",
    "        \"\"\"\n",
    "        weight_del = e * weight_grad + a * self.last_delta_grads[0]\n",
    "        self.weights -= weight_del\n",
    "        bias_del = e * bias_grad + a * self.last_delta_grads[1]\n",
    "        self.biases -= bias_del\n",
    "        self.last_delta_grads = weight_del, bias_del\n",
    "\n",
    "    def forward(self, in_vec): \n",
    "        \"\"\"Forward pass of the neural network\n",
    "\n",
    "        Args:\n",
    "            in_vec (np.array): binary vector of length 6\n",
    "\n",
    "        Returns:\n",
    "            tuple: output values for neurons 6, 7, and 8. 8 is the final output of the network\n",
    "        \"\"\"\n",
    "        x_6 = input(in_vec, self.weights[:6], self.biases[0])\n",
    "        x_7 = input(in_vec, self.weights[6:12], self.biases[1])\n",
    "        y_6 = output(x_6)\n",
    "        y_7 = output(x_7)\n",
    "        x_8 = input(np.array((y_6, y_7)), self.weights[12:], self.biases[2])\n",
    "        y_8 = output(x_8)\n",
    "        return y_6, y_7, y_8\n",
    "     \n",
    "    def backward(self, in_vec, y_6, y_7, y_8, y_true):\n",
    "        \"\"\"Backpropagation for the neural net. \n",
    "\n",
    "        Args:\n",
    "            in_vec (np.array): Output of neurons 0-5\n",
    "            y_6 (float): Output of neuron 6\n",
    "            y_7 (float): Output of neuron 7\n",
    "            y_8 (float): Output of neuron 8\n",
    "            y_true (float): Ground truth value for input vector\n",
    "\n",
    "        Returns:\n",
    "            tuple: partial derivative w.r.t. each weight and each bias\n",
    "        \"\"\"\n",
    "        weight_partials = np.zeros(14)\n",
    "        bias_partials = np.zeros(3)\n",
    "\n",
    "        # Partials for neuron 8\n",
    "        partial_y_8 = y_8 - y_true\n",
    "        partial_x_8 = partial_y_8 * y_8 * (1 - y_8)\n",
    "\n",
    "        # partials for second layer of weights and biases\n",
    "        weight_partials[13] = partial_x_8 * y_7\n",
    "        weight_partials[12] = partial_x_8 * y_6\n",
    "        bias_partials[2] = partial_x_8\n",
    "\n",
    "        # Partials for neurons 6 and 7\n",
    "        partial_y_7 = partial_x_8 * self.weights[13]\n",
    "        partial_x_7 = partial_y_7 * y_7 * (1 - y_7)\n",
    "        partial_y_6 = partial_x_8 * self.weights[12]\n",
    "        partial_x_6 = partial_y_6 * (y_6) * (1 - y_6)\n",
    "\n",
    "        # Partials for first layer of weights and biases\n",
    "        weight_partials[6:12] = partial_x_7 * in_vec\n",
    "        bias_partials[1] = partial_x_7\n",
    "        weight_partials[:6] = partial_x_6 * in_vec\n",
    "        bias_partials[0] = partial_x_6\n",
    "\n",
    "        return weight_partials, bias_partials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Generate dataset\n",
    "X = np.array(list(itertools.product([0, 1], repeat=6)))\n",
    "y = np.zeros((64, 1))\n",
    "\n",
    "for i, (in_vec) in enumerate(X):\n",
    "    if (in_vec[:3] == in_vec[::-1][:3]).all():\n",
    "        y[i] = 1\n",
    "\n",
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our network\n",
    "net = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\340863302.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  epoch_correct += float(abs(y_8 - y[i]) < .2)\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\2741535366.py:63: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  weight_partials[13] = partial_x_8 * y_7\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\2741535366.py:64: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  weight_partials[12] = partial_x_8 * y_6\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\2741535366.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bias_partials[2] = partial_x_8\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\2741535366.py:75: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bias_partials[1] = partial_x_7\n",
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_28744\\2741535366.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bias_partials[0] = partial_x_6\n",
      " 85%|████████▍ | 846/1000 [00:01<00:00, 738.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the training loop\n",
    "# The paper states 1425 sweeps (epochs) needed to achieve a perfect score\n",
    "\n",
    "loss, accuracy = [], []\n",
    "for epoch in tqdm(range(1000)):\n",
    "    # Track cumulative error, correct guesses, weight gradient sum, batch gradient sum\n",
    "    epoch_error = 0\n",
    "    epoch_correct = 0\n",
    "    batch_weight = np.zeros(14)\n",
    "    batch_bias = np.zeros(3)\n",
    "\n",
    "    # Loop over 64 vectors in training set\n",
    "    for i, in_vec in enumerate(X):\n",
    "        y_6, y_7, y_8 = net.forward(in_vec)\n",
    "        \n",
    "        epoch_error += (y_8 - y[i]) ** 2\n",
    "        epoch_correct += float(abs(y_8 - y[i]) < .2)\n",
    "        \n",
    "        weight_p, bias_p = net.backward(in_vec, y_6, y_7, y_8, y[i])\n",
    "        \n",
    "        batch_weight += weight_p\n",
    "        batch_bias += bias_p\n",
    "\n",
    "    # Update weights and biases\n",
    "    net.update_weights(batch_weight, batch_bias)\n",
    "    # Track training metrics\n",
    "    loss.append(epoch_error/64)\n",
    "    accuracy.append(epoch_correct/64)\n",
    "    \n",
    "    if accuracy[-1] == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training breakdown\n",
    "\n",
    "The network took 846 sweeps to learn whether a length 6 vector was mirrored. I set the threshold for correct classification at .2. Another option would be to consider all predictions < .5 as 0 and >= .5 as 1. This would likely result in a shorter training time. Looking at the loss and accuracy below, the shape of each curve is contrary to what I have seen in the past. The accuracy dips late in training, before bouncing around and reaching 1. The loss looks more familiar, however it seems to bounce a bit before leveling off. This could be because of the exponential decay term in the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21389e5fd10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(accuracy, label = 'accuracy')\n",
    "plt.plot(loss, label = 'loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.13520609,  4.23356045, -8.39599728,  8.3946728 , -4.2355389 ,\n",
       "         2.13277956]),\n",
       " array([ 2.1556448 , -4.27959344,  8.48300179, -8.48416879,  4.27795855,\n",
       "        -2.15733541]),\n",
       " array([-10.34858855, -10.31945361]),\n",
       " array([-1.78081604, -1.79780053,  4.36729403]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[:6], net.weights[6:12], net.weights[12:], net.biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
