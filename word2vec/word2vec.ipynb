{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh77YdjXyI/Z9lRUerZdg5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Estimation of Word Representations in Vector Space\n",
        "## Authors: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n",
        "### Notes: Michael Holtz"
      ],
      "metadata": {
        "id": "KQTGeqGJG75O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract\n",
        "\n",
        "They describe two NN architectures for embedding words into vector spaces. The quality is measured via a \"word similarity task,\" and they find higher accuracy at much lower computational cost. Furthermore, the vectors produced provide state-of-the-art performance on a test set for syntactic and semantic word similarities."
      ],
      "metadata": {
        "id": "5tgwoMWcHkLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro\n",
        "\n",
        "Many previous NLP systems treat words as simply an element in a set of words, with no notion of similarity between words. These simple techniques have notable limits in many tasks. While trillions of words might be necessary to achieve performance with these simple methods, tasks such as automatic speech recognition or machine translation may have corpora with only millions or billions of words. In these scenarios, a more complex strategy is needed."
      ],
      "metadata": {
        "id": "YaN5LZiRInPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper Goals\n",
        "\n",
        "The goal is to create high-quality vector embeddings for millions of words from a billion+ word corpora. The expectation of the embedding is that similar words should be close to one another and that words can have multiple degrees of similarity (such as a similar ending). More surprising is that algebraic operations on these vectors hold their meaning. Ex. King - man + woman = queen. They also develop a test set for syntactic and semantic regularities and discuss how time and accuracy depend on embedding dimension.\n"
      ],
      "metadata": {
        "id": "vNfnsdAMKEyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aV-9QV_pVdzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous work\n",
        "\n",
        "Previous attempts at word embeddings via a neural network language model. The first proposed models learned both a word vector representation and a statistical language model. Later architectures attempted to learn the embedding via a single hidden layer and then the vectors were used to train the NNLM. Other work also found that NLP tasks became easier when working with word vectors. The architecture in this paper seeks to find these vectors in a much more computationally efficient way."
      ],
      "metadata": {
        "id": "XSCUS6fSREXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NNLM Architectures\n",
        "\n",
        "#### Feedforward NNLM\n",
        "This model takes in N-words encoded in one-of-V coding. The input layer is then projected to a projection layer P. This layer is passed to a hidden layer, which in turn predicts a probability distribution over the 1xV output layer.\n",
        "\n",
        "#### Recurrent NNLM (RNNLM)\n",
        "This model removes the need to specify the context length for the input. The RNNLM removes the projection layer, consisting of only input, hidden, and output layers. It is a recurrent architecture becuase there are time delayed connections from the hidden layer to itself. These connection theoretically allow for short term memory, allowing past words to influence future predictions.\n"
      ],
      "metadata": {
        "id": "M9Jr35UpTzyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New log-linear models\n",
        "The main focus of the paper. New models are proposed which avoid the nonlinear nature of the neural nets above, allowing for much more efficient training. These new models can then be used to train the above architectures on a much smaller input dimension.\n",
        "\n",
        "#### CBOW\n",
        "Continous bag of words (CBOW) is similar to the feedforward model but there is no non-linear hidden layer. Instead the projection layer is shared for all words, and input contains words from the past as well as the future. The goal is to classify the word in the middle of the input."
      ],
      "metadata": {
        "id": "lECIF9ipdl8x"
      }
    }
  ]
}