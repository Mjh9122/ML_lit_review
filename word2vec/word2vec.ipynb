{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mjh9122/ML_lit_review/blob/main/word2vec/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQTGeqGJG75O"
   },
   "source": [
    "# Efficient Estimation of Word Representations in Vector Space\n",
    "## Authors: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n",
    "### Notes: Michael Holtz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tgwoMWcHkLN"
   },
   "source": [
    "### Abstract\n",
    "\n",
    "They describe two NN architectures for embedding words into vector spaces. The quality is measured via a \"word similarity task,\" and they find higher accuracy at much lower computational cost. Furthermore, the vectors produced provide state-of-the-art performance on a test set for syntactic and semantic word similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaN5LZiRInPM"
   },
   "source": [
    "### Intro\n",
    "\n",
    "Many previous NLP systems treat words as simply an element in a set of words, with no notion of similarity between words. These simple techniques have notable limits in many tasks. While trillions of words might be necessary to achieve performance with these simple methods, tasks such as automatic speech recognition or machine translation may have corpora with only millions or billions of words. In these scenarios, a more complex strategy is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNfnsdAMKEyM"
   },
   "source": [
    "### Paper Goals\n",
    "\n",
    "The goal is to create high-quality vector embeddings for millions of words from a billion+ word corpora. The expectation of the embedding is that similar words should be close to one another and that words can have multiple degrees of similarity (such as a similar ending). More surprising is that algebraic operations on these vectors hold their meaning. Ex. King - man + woman = queen. They also develop a test set for syntactic and semantic regularities and discuss how time and accuracy depend on embedding dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV-9QV_pVdzK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSCUS6fSREXK"
   },
   "source": [
    "### Previous work\n",
    "\n",
    "Previous attempts at word embeddings via a neural network language model. The first proposed models learned both a word vector representation and a statistical language model. Later architectures attempted to learn the embedding via a single hidden layer and then the vectors were used to train the NNLM. Other work also found that NLP tasks became easier when working with word vectors. The architecture in this paper seeks to find these vectors in a much more computationally efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Jr35UpTzyS"
   },
   "source": [
    "### NNLM Architectures\n",
    "\n",
    "#### Feedforward NNLM\n",
    "This model takes in N-words encoded in one-of-V coding. The input layer is then projected to a projection layer P. This layer is passed to a hidden layer, which in turn predicts a probability distribution over the 1xV output layer.\n",
    "\n",
    "#### Recurrent NNLM (RNNLM)\n",
    "This model removes the need to specify the context length for the input. The RNNLM removes the projection layer, consisting of only input, hidden, and output layers. It is a recurrent architecture becuase there are time delayed connections from the hidden layer to itself. These connection theoretically allow for short term memory, allowing past words to influence future predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lECIF9ipdl8x"
   },
   "source": [
    "### New log-linear models\n",
    "The main focus of the paper. New models are proposed which avoid the nonlinear nature of the neural nets above, allowing for much more efficient training. These new models can then be used to train the above architectures on a much smaller input dimension.\n",
    "\n",
    "#### CBOW\n",
    "Continous bag of words (CBOW) is similar to the feedforward model but there is no non-linear hidden layer. Instead the projection layer is shared for all words, and input contains words from the past as well as the future. The goal is to classify the word in the middle of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "p9alwnmwrIe_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter('runs')\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import corpus and build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zA7ZxQ5cr5l_"
   },
   "outputs": [],
   "source": [
    "with open('text8', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "corpus = [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYJcjQEMr0Bf",
    "outputId": "79ab11cc-cbef-4045-a9d5-3f369736baa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 833184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 663/124301818 [00:02<129:18:32, 267.02it/s]"
     ]
    }
   ],
   "source": [
    "# Create vocab\n",
    "vocab = set(corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size: {vocab_size}')\n",
    "\n",
    "# To convert from word to embedding index\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "# Create dataset by taking four words on either side of a target word\n",
    "context_length = 4\n",
    "contexts, targets = [], []\n",
    "\n",
    "for i in tqdm(range(context_length, len(corpus) - context_length)):\n",
    "    context = corpus[i - context_length: i] + corpus[i + 1 : i + context_length + 1]\n",
    "    target = corpus[i]\n",
    "    targets.append(target)\n",
    "    contexts.append(context)\n",
    "\n",
    "\n",
    "CBOW_Xs = torch.tensor([[word_to_ix[w] for w in x] for x in contexts])\n",
    "CBOW_ys = torch.tensor([word_to_ix[y] for y in targets])\n",
    "SKIP_Xs, SKIP_ys = [], []\n",
    "\n",
    "# for context, target in tqdm(zip(contexts, targets), total = len(targets)):\n",
    "#     for ctxt_word in context:\n",
    "#         SKIP_Xs.append(word_to_ix[target])\n",
    "#         SKIP_ys.append(word_to_ix[ctxt_word])\n",
    "# \n",
    "# SKIP_Xs = torch.tensor(SKIP_Xs)\n",
    "# SKIP_ys = torch.tensor(SKIP_ys)\n",
    "# \n",
    "# print(f'CBOW training length: {CBOW_Xs.shape} Skipgram training length: {SKIP_ys.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wah-YppZ8o53"
   },
   "outputs": [],
   "source": [
    "# Quick dataset class for dataloading\n",
    "class Simple_Dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Y[idx],self.X[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define CBOW and Skipgram classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5chQGEj9rNUP"
   },
   "outputs": [],
   "source": [
    "# CBOW model proper. No non-linear activations like the paper says.\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.tensor(word_to_ix[word], device=self.device)\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIPGRAM model. No non-linear activations. No negative sampling. \n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(-1, 1)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.tensor(word_to_ix[word])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate CBOW, loss, optim, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899992"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create cbow instance, loss, optimizer, dataset, and dataloader\n",
    "batch_size = 256\n",
    "\n",
    "cbow_model = CBOW(vocab_size, 256)\n",
    "cbow_model.to(device)\n",
    "cbow_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "cbow_optim = torch.optim.Adam(cbow_model.parameters())\n",
    "\n",
    "CBOW_X_train, CBOW_X_test, CBOW_y_train, CBOW_y_test = train_test_split(CBOW_Xs, CBOW_ys, test_size = .1)\n",
    "cbow_train = Simple_Dataset(CBOW_X_train, CBOW_y_train)\n",
    "cbow_test = Simple_Dataset(CBOW_X_test, CBOW_y_test)\n",
    "cbow_train_dataloader = torch.utils.data.DataLoader(cbow_train, batch_size=batch_size, shuffle=True)\n",
    "cbow_test_dataloader = torch.utils.data.DataLoader(cbow_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len(cbow_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ3UAOgIvkow",
    "outputId": "4f9c7579-d213-4a8f-ef0e-0b6eee68161c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3516/3516 [00:55<00:00, 63.42it/s]\n",
      " 11%|█         | 391/3516 [00:01<00:08, 353.86it/s]\n",
      "100%|██████████| 3516/3516 [00:58<00:00, 59.73it/s]\n",
      " 11%|█         | 391/3516 [00:01<00:08, 358.49it/s]\n",
      "  5%|▌         | 180/3516 [00:02<00:53, 61.92it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    running_correct = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    cbow_model.train()\n",
    "    for i, (labels, features) in tqdm(enumerate(cbow_train_dataloader), total = len(cbow_train_dataloader)):\n",
    "        labels, features = labels.to(device), features.to(device)\n",
    "        cbow_optim.zero_grad()\n",
    "        y_pred = cbow_model(features)\n",
    "        loss = cbow_loss(y_pred, labels)\n",
    "        loss.backward()\n",
    "        cbow_optim.step()\n",
    "\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 99 == 0:\n",
    "            writer.add_scalar('training batch loss', running_loss/(batch_size * 100), epoch * len(cbow_train_dataloader) + i)\n",
    "            running_loss = 0\n",
    "\n",
    "    writer.add_scalar('train accuracy', running_correct/len(cbow_train), epoch)\n",
    "    running_correct = 0\n",
    "\n",
    "    cbow_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, features)in tqdm(enumerate(cbow_test_dataloader),  total = len(cbow_test_dataloader)):\n",
    "            labels, features = labels.to(device), features.to(device)\n",
    "            y_pred = cbow_model(features)\n",
    "            preds = torch.argmax(y_pred, dim=1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "    \n",
    "    writer.add_scalar('test accuracy', running_correct/len(cbow_test), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.06784402]], dtype=float32), array([[0.0668553]], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = cbow_model.get_word_embedding('king').cpu().detach().numpy()\n",
    "woman = cbow_model.get_word_embedding('woman').cpu().detach().numpy() \n",
    "man = cbow_model.get_word_embedding('man').cpu().detach().numpy()\n",
    "queen = cbow_model.get_word_embedding('queen').cpu().detach().numpy() \n",
    "cosine_similarity(king, queen), cosine_similarity(man, woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03422299]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(queen, king - man + woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZ9cqSkiTx5OhJT13U+dq1",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
