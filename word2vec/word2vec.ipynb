{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgF83ia6eUlvnSq+0Q6fKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mjh9122/ML_lit_review/blob/main/word2vec/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Estimation of Word Representations in Vector Space\n",
        "## Authors: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n",
        "### Notes: Michael Holtz"
      ],
      "metadata": {
        "id": "KQTGeqGJG75O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract\n",
        "\n",
        "They describe two NN architectures for embedding words into vector spaces. The quality is measured via a \"word similarity task,\" and they find higher accuracy at much lower computational cost. Furthermore, the vectors produced provide state-of-the-art performance on a test set for syntactic and semantic word similarities."
      ],
      "metadata": {
        "id": "5tgwoMWcHkLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro\n",
        "\n",
        "Many previous NLP systems treat words as simply an element in a set of words, with no notion of similarity between words. These simple techniques have notable limits in many tasks. While trillions of words might be necessary to achieve performance with these simple methods, tasks such as automatic speech recognition or machine translation may have corpora with only millions or billions of words. In these scenarios, a more complex strategy is needed."
      ],
      "metadata": {
        "id": "YaN5LZiRInPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper Goals\n",
        "\n",
        "The goal is to create high-quality vector embeddings for millions of words from a billion+ word corpora. The expectation of the embedding is that similar words should be close to one another and that words can have multiple degrees of similarity (such as a similar ending). More surprising is that algebraic operations on these vectors hold their meaning. Ex. King - man + woman = queen. They also develop a test set for syntactic and semantic regularities and discuss how time and accuracy depend on embedding dimension.\n"
      ],
      "metadata": {
        "id": "vNfnsdAMKEyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aV-9QV_pVdzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous work\n",
        "\n",
        "Previous attempts at word embeddings via a neural network language model. The first proposed models learned both a word vector representation and a statistical language model. Later architectures attempted to learn the embedding via a single hidden layer and then the vectors were used to train the NNLM. Other work also found that NLP tasks became easier when working with word vectors. The architecture in this paper seeks to find these vectors in a much more computationally efficient way."
      ],
      "metadata": {
        "id": "XSCUS6fSREXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NNLM Architectures\n",
        "\n",
        "#### Feedforward NNLM\n",
        "This model takes in N-words encoded in one-of-V coding. The input layer is then projected to a projection layer P. This layer is passed to a hidden layer, which in turn predicts a probability distribution over the 1xV output layer.\n",
        "\n",
        "#### Recurrent NNLM (RNNLM)\n",
        "This model removes the need to specify the context length for the input. The RNNLM removes the projection layer, consisting of only input, hidden, and output layers. It is a recurrent architecture becuase there are time delayed connections from the hidden layer to itself. These connection theoretically allow for short term memory, allowing past words to influence future predictions.\n"
      ],
      "metadata": {
        "id": "M9Jr35UpTzyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New log-linear models\n",
        "The main focus of the paper. New models are proposed which avoid the nonlinear nature of the neural nets above, allowing for much more efficient training. These new models can then be used to train the above architectures on a much smaller input dimension.\n",
        "\n",
        "#### CBOW\n",
        "Continous bag of words (CBOW) is similar to the feedforward model but there is no non-linear hidden layer. Instead the projection layer is shared for all words, and input contains words from the past as well as the future. The goal is to classify the word in the middle of the input."
      ],
      "metadata": {
        "id": "lECIF9ipdl8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p9alwnmwrIe_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Corpus\n",
        "corpus = \"\"\"\n",
        "The Cat in the Hat\n",
        "\n",
        "By Dr. Seuss\n",
        "\n",
        "The sun did not shine.\n",
        "It was too wet to play.\n",
        "So we sat in the house\n",
        "All that cold, cold, wet day.\n",
        "\n",
        "I sat there with Sally.\n",
        "We sat there, we two.\n",
        "And I said, \"How I wish\n",
        "We had something to do!\"\n",
        "\n",
        "Too wet to go out\n",
        "And too cold to play ball.\n",
        "So we sat in the house.\n",
        "We did nothing at all.\n",
        "\n",
        "So all we could do was to\n",
        "\n",
        "Sit!\n",
        "Sit!\n",
        "Sit!\n",
        "Sit!\n",
        "\n",
        "And we did not like it.\n",
        "Not one little bit.\n",
        "\n",
        "BUMP!\n",
        "\n",
        "And then\n",
        "something went BUMP!\n",
        "How that bump made us jump!\n",
        "\n",
        "We looked!\n",
        "Then we saw him step in on the mat!\n",
        "We looked!\n",
        "And we saw him!\n",
        "The Cat in the Hat!\n",
        "And he said to us,\n",
        "\"Why do you sit there like that?\"\n",
        "\"I know it is wet\n",
        "And the sun is not sunny.\n",
        "But we can have\n",
        "Lots of good fun that is funny!\"\n",
        "\n",
        "\"I know some good games we could play,\"\n",
        "Said the cat.\n",
        "\"I know some new tricks,\"\n",
        "Said the Cat in the Hat.\n",
        "\"A lot of good tricks.\n",
        "I will show them to you.\n",
        "Your mother\n",
        "Will not mind at all if I do.\"\n",
        "\n",
        "Then Sally and I\n",
        "Did not know what to say.\n",
        "Our mother was out of the house\n",
        "For the day.\n",
        "\n",
        "But our fish said, \"No! No!\n",
        "Make that cat go away!\n",
        "Tell that Cat in the Hat\n",
        "You do NOT want to play.\n",
        "He should not be here.\n",
        "He should not be about.\n",
        "He should not be here\n",
        "When your mother is out!\"\n",
        "\n",
        "\"Now! Now! Have no fear.\n",
        "Have no fear!\" said the cat.\n",
        "\"My tricks are not bad,\"\n",
        "Said the Cat in the Hat.\n",
        "\"Why, we can have\n",
        "Lots of good fun, if you wish,\n",
        "with a game that I call\n",
        "UP-UP-UP with a fish!\"\n",
        "\n",
        "\"Put me down!\" said the fish.\n",
        "\"This is no fun at all!\n",
        "Put me down!\" said the fish.\n",
        "\"I do NOT wish to fall!\"\n",
        "\n",
        "\"Have no fear!\" said the cat.\n",
        "\"I will not let you fall.\n",
        "I will hold you up high\n",
        "As I stand on a ball.\n",
        "With a book on one hand!\n",
        "And a cup on my hat!\n",
        "But that is not ALL I can do!\"\n",
        "Said the cat...\n",
        "\n",
        "\"Look at me!\n",
        "Look at me now!\" said the cat.\n",
        "\"With a cup and a cake\n",
        "On the top of my hat!\n",
        "I can hold up TWO books!\n",
        "I can hold up the fish!\n",
        "And a litte toy ship!\n",
        "And some milk on a dish!\n",
        "And look!\n",
        "I can hop up and down on the ball!\n",
        "But that is not all!\n",
        "Oh, no.\n",
        "That is not all...\n",
        "\n",
        "\"Look at me!\n",
        "Look at me!\n",
        "Look at me NOW!\n",
        "It is fun to have fun\n",
        "But you have to know how.\n",
        "I can hold up the cup\n",
        "And the milk and the cake!\n",
        "I can hold up these books!\n",
        "And the fish on a rake!\n",
        "I can hold the toy ship\n",
        "And a little toy man!\n",
        "And look! With my tail\n",
        "I can hold a red fan!\n",
        "I can fan with the fan\n",
        "As I hop on the ball!\n",
        "But that is not all.\n",
        "Oh, no.\n",
        "That is not all....\"\n",
        "\n",
        "That is what the cat said...\n",
        "Then he fell on his head!\n",
        "He came down with a bump\n",
        "From up there on the ball.\n",
        "And Sally and I,\n",
        "We saw ALL the things fall!\n",
        "\n",
        "And our fish came down, too.\n",
        "He fell into a pot!\n",
        "He said, \"Do I like this?\"\n",
        "Oh, no! I do not.\n",
        "This is not a good game,\"\n",
        "Said our fish as he lit.\n",
        "\"No, I do not like it,\n",
        "Not one little bit!\"\n",
        "\n",
        "\"Now look what you did!\"\n",
        "Said the fish to the cat.\n",
        "\"Now look at this house!\n",
        "Look at this! Look at that!\n",
        "You sank our toy ship,\n",
        "Sank it deep in the cake.\n",
        "You shook up our house\n",
        "And you bent our new rake.\n",
        "You SHOULD NOT be here\n",
        "When our mother is not.\n",
        "You get out of this house!\"\n",
        "Said the fish in the pot.\n",
        "\n",
        "\"But I like to be here.\n",
        "Oh, I like it a lot!\"\n",
        "Said the Cat in the Hat\n",
        "To the fish in the pot.\n",
        "\"I will NOT go away.\n",
        "I do NOT wish to go!\n",
        "And so,\" said the Cat in the Hat,\n",
        "\n",
        "\"So\n",
        "so\n",
        "so...\n",
        "\n",
        "I will show you\n",
        "Another good game that I know!\"\n",
        "And then he ran out.\n",
        "And, then, fast as a fox,\n",
        "The Cat in the Hat\n",
        "Came back in with a box.\n",
        "A big red wood box.\n",
        "It was shut with a hook.\n",
        "\"Now look at this trick,\"\n",
        "Said the cat.\n",
        "\"Take a look!\"\n",
        "\n",
        "Then he got up on top\n",
        "With a tip of his hat.\n",
        "\"I call this game FUN-IN-A-BOX,\"\n",
        "Said the cat.\n",
        "\"In this box are two things\n",
        "I will show to you now.\n",
        "You will like these two things,\"\n",
        "Said the cat with a bow.\n",
        "\n",
        "\"I will pick up the hook.\n",
        "You will see something new.\n",
        "Two things. And I call them\n",
        "Thing One and Thing Two.\n",
        "These Things will not bite you.\n",
        "They want to have fun.\"\n",
        "Then, out of the box\n",
        "Came Thing Two and Thing One!\n",
        "And they ran to us fast.\n",
        "They said, \"How do you do?\n",
        "Would you like to shake hands\n",
        "With Thing One and Thing Two?\"\n",
        "\n",
        "And Sally and I\n",
        "Did not know what to do.\n",
        "So we had to shake hands\n",
        "With Thing One and Thing Two.\n",
        "We shook their two hands.\n",
        "But our fish said, \"No! No!\n",
        "Those Things should not be\n",
        "In this house! Make them go!\n",
        "\"They should not be here\n",
        "When your mother is not!\n",
        "Put them out! Put them out!\"\n",
        "Said the fish in the pot.\n",
        "\n",
        "\"Have no fear, little fish,\"\n",
        "Said the Cat in the Hat.\n",
        "\"These Things are good Things.\"\n",
        "And he gave them a pat.\n",
        "\"They are tame. Oh, so tame!\n",
        "They have come here to play.\n",
        "They will give you some fun\n",
        "On this wet, wet, wet day.\"\n",
        "\n",
        "\"Now, here is a game that they like,\"\n",
        "Said the cat.\n",
        "\"They like to fly kites,\"\n",
        "Said the Cat in the Hat\n",
        "\n",
        "\"No! Not in the house!\"\n",
        "Said the fish in the pot.\n",
        "\"They should not fly kites\n",
        "In a house! They should not.\n",
        "Oh, the things they will bump!\n",
        "Oh, the things they will hit!\n",
        "Oh, I do not like it!\n",
        "Not one little bit!\" Then Sally and I\n",
        "Saw them run down the hall.\n",
        "We saw those two Things\n",
        "Bump their kites on the wall!\n",
        "Bump! Thump! Thump! Bump!\n",
        "Down the wall in the hall.\n",
        "\n",
        "Thing Two and Thing One!\n",
        "They ran up! They ran down!\n",
        "On the string of one kite\n",
        "We saw Mother's new gown!\n",
        "Her gown with the dots\n",
        "That are pink, white and red.\n",
        "Then we saw one kite bump\n",
        "On the head of her bed!\n",
        "\n",
        "Then those Things ran about\n",
        "With big bumps, jumps and kicks\n",
        "And with hops and big thumps\n",
        "And all kinds of bad tricks.\n",
        "And I said,\n",
        "\"I do NOT like the way that they play\n",
        "If Mother could see this,\n",
        "Oh, what would she say!\"\n",
        "\n",
        "Then our fish said, \"Look! Look!\"\n",
        "And our fish shook with fear.\n",
        "\"Your mother is on her way home!\n",
        "Do you hear?\n",
        "Oh, what will she do to us?\n",
        "What will she say?\n",
        "Oh, she will not like it\n",
        "To find us this way!\"\n",
        "\n",
        "\"So, DO something! Fast!\" said the fish.\n",
        "\"Do you hear!\n",
        "I saw her. Your mother!\n",
        "Your mother is near!\n",
        "So, as fast as you can,\n",
        "Think of something to do!\n",
        "You will have to get rid of\n",
        "Thing One and Thing Two!\"\n",
        "\n",
        "So, as fast as I could,\n",
        "I went after my net.\n",
        "And I said, \"With my net\n",
        "I can get them I bet.\n",
        "I bet, with my net,\n",
        "I can get those Things yet!\"\n",
        "\n",
        "Then I let down my net.\n",
        "It came down with a PLOP!\n",
        "And I had them! At last!\n",
        "Thoe two Things had to stop.\n",
        "Then I said to the cat,\n",
        "\"Now you do as I say.\n",
        "You pack up those Things\n",
        "And you take them away!\"\n",
        "\n",
        "\"Oh dear!\" said the cat,\n",
        "\"You did not like our game...\n",
        "Oh dear.\n",
        "\n",
        "What a shame!\n",
        "What a shame!\n",
        "What a shame!\"\n",
        "\n",
        "Then he shut up the Things\n",
        "In the box with the hook.\n",
        "And the cat went away\n",
        "With a sad kind of look.\n",
        "\n",
        "\"That is good,\" said the fish.\n",
        "\"He has gone away. Yes.\n",
        "But your mother will come.\n",
        "She will find this big mess!\n",
        "And this mess is so big\n",
        "And so deep and so tall,\n",
        "We ca not pick it up.\n",
        "There is no way at all!\"\n",
        "\n",
        "And THEN!\n",
        "Who was back in the house?\n",
        "Why, the cat!\n",
        "\"Have no fear of this mess,\"\n",
        "Said the Cat in the Hat.\n",
        "\"I always pick up all my playthings\n",
        "And so...\n",
        "I will show you another\n",
        "Good trick that I know!\"\n",
        "\n",
        "Then we saw him pick up\n",
        "All the things that were down.\n",
        "He picked up the cake,\n",
        "And the rake, and the gown,\n",
        "And the milk, and the strings,\n",
        "And the books, and the dish,\n",
        "And the fan, and the cup,\n",
        "And the ship, and the fish.\n",
        "And he put them away.\n",
        "Then he said, \"That is that.\"\n",
        "And then he was gone\n",
        "With a tip of his hat.\n",
        "\n",
        "Then our mother came in\n",
        "And she said to us two,\n",
        "\"Did you have any fun?\n",
        "Tell me. What did you do?\"\n",
        "\n",
        "And Sally and I did not know\n",
        "What to say.\n",
        "Should we tell her\n",
        "The things that went on there that day?\n",
        "\n",
        "Should we tell her about it?\n",
        "Now, what SHOULD we do?\n",
        "Well...\n",
        "What would YOU do\n",
        "If your mother asked YOU?\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "corpus_filtered = re.sub('[^A-Za-z0-9 ]+', \" \", corpus)\n",
        "corpus = corpus_filtered.lower().split()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zA7ZxQ5cr5l_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab\n",
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)\n",
        "print(f'vocab size: {vocab_size}')\n",
        "\n",
        "# To convert from word to embedding index\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "# Create dataset by taking four words on either side of a target word\n",
        "context_length = 4\n",
        "\n",
        "Xs = []\n",
        "ys = []\n",
        "for i in range(context_length, len(corpus) - context_length):\n",
        "    context = corpus[i - context_length: i] + corpus[i + 1 : i + context_length + 1]\n",
        "    target = corpus[i]\n",
        "    Xs.append(context)\n",
        "    ys.append(target)\n",
        "\n",
        "Xs_t = [torch.tensor([word_to_ix[w] for w in x]) for x in Xs]\n",
        "ys_t = [torch.tensor([word_to_ix[y]]) for y in ys]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYJcjQEMr0Bf",
        "outputId": "b18bd13d-640a-4de7-c6be-81ee2f98e767"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick dataset class for dataloading\n",
        "class CBOW_Dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.Y[idx],self.X[idx]"
      ],
      "metadata": {
        "id": "Wah-YppZ8o53"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW model proper. No non-linear activations like the paper says.\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).sum(dim=1)\n",
        "        out = self.linear(embeds)\n",
        "        return self.softmax(out)\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor(word_to_ix[word])\n",
        "        return self.embeddings(word).view(1,-1)"
      ],
      "metadata": {
        "id": "5chQGEj9rNUP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model instance, loss, optimizer, dataset, and dataloader\n",
        "model = CBOW(vocab_size, 64)\n",
        "loss_function = nn.NLLLoss()\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "dataset = CBOW_Dataset(Xs_t, ys_t)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1000):\n",
        "    epoch_loss = 0\n",
        "    iters = 0\n",
        "    correct = 0\n",
        "\n",
        "    for labels, features in dataloader:\n",
        "        optim.zero_grad()\n",
        "        y_pred = model(features)\n",
        "        correct += sum(torch.argmax(y_pred, dim=1) == labels.squeeze())\n",
        "        loss = loss_function(y_pred, labels.squeeze())\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        epoch_loss += loss\n",
        "        iters += 1\n",
        "\n",
        "    if not epoch % 50:\n",
        "        print(f'Epoch: {epoch}, Loss {epoch_loss.item()/iters}, Accuracy: {correct/len(Xs_t)}, Incorrect classifications: {(len(Xs_t) - correct)}/{len(Xs_t)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ3UAOgIvkow",
        "outputId": "b798a825-b873-4e25-c2c3-46da118ddc64"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss 6.716022491455078, Accuracy: 0.012907191179692745, Incorrect classifications: 1606/1627\n",
            "Epoch: 50, Loss 1.8378186907087053, Accuracy: 0.563614010810852, Incorrect classifications: 710/1627\n",
            "Epoch: 100, Loss 1.2621653420584542, Accuracy: 0.7277197241783142, Incorrect classifications: 443/1627\n",
            "Epoch: 150, Loss 0.9797385079520089, Accuracy: 0.8014751076698303, Incorrect classifications: 323/1627\n",
            "Epoch: 200, Loss 0.8073596954345703, Accuracy: 0.8469575643539429, Incorrect classifications: 249/1627\n",
            "Epoch: 250, Loss 0.6417697497776577, Accuracy: 0.882606029510498, Incorrect classifications: 191/1627\n",
            "Epoch: 300, Loss 0.5369910853249686, Accuracy: 0.9108788967132568, Incorrect classifications: 145/1627\n",
            "Epoch: 350, Loss 0.4448422704424177, Accuracy: 0.9397664666175842, Incorrect classifications: 98/1627\n",
            "Epoch: 400, Loss 0.3771602766854422, Accuracy: 0.9508297443389893, Incorrect classifications: 80/1627\n",
            "Epoch: 450, Loss 0.30984534536089214, Accuracy: 0.9674246907234192, Incorrect classifications: 53/1627\n",
            "Epoch: 500, Loss 0.27370006697518484, Accuracy: 0.9772587418556213, Incorrect classifications: 37/1627\n",
            "Epoch: 550, Loss 0.22301459312438965, Accuracy: 0.9840196967124939, Incorrect classifications: 26/1627\n",
            "Epoch: 600, Loss 0.19417195660727365, Accuracy: 0.9883220791816711, Incorrect classifications: 19/1627\n",
            "Epoch: 650, Loss 0.17026841640472412, Accuracy: 0.9907805919647217, Incorrect classifications: 15/1627\n",
            "Epoch: 700, Loss 0.15219501086643764, Accuracy: 0.9932391047477722, Incorrect classifications: 11/1627\n",
            "Epoch: 750, Loss 0.13437363079616002, Accuracy: 0.9956976175308228, Incorrect classifications: 7/1627\n",
            "Epoch: 800, Loss 0.11767217942646571, Accuracy: 0.9963122606277466, Incorrect classifications: 6/1627\n",
            "Epoch: 850, Loss 0.1060053961617606, Accuracy: 0.9975414872169495, Incorrect classifications: 4/1627\n",
            "Epoch: 900, Loss 0.09787113325936454, Accuracy: 0.9987707734107971, Incorrect classifications: 2/1627\n",
            "Epoch: 950, Loss 0.08910755600248065, Accuracy: 0.9987707734107971, Incorrect classifications: 2/1627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on a sample sentance from the text\n",
        "test = 'and then something went bump how that made us'\n",
        "test = test.lower().split()\n",
        "context = test[:4] + test[5:]\n",
        "target = test[4]\n",
        "\n",
        "test_vec = torch.tensor([word_to_ix[w] for w in context]).unsqueeze(0)\n",
        "log_probs = model(test_vec)\n",
        "pred = ix_to_word[torch.argmax(log_probs).item()]\n",
        "\n",
        "print(f'Test context: {context}')\n",
        "print(f'Test target: {target}')\n",
        "print(f'Test prediction: {pred}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs6RkziFwxsA",
        "outputId": "5e8f488a-0558-4d9e-c1db-af59df8b5e70"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test context: ['and', 'then', 'something', 'went', 'how', 'that', 'made', 'us']\n",
            "Test target: bump\n",
            "Test prediction: bump\n"
          ]
        }
      ]
    }
  ]
}